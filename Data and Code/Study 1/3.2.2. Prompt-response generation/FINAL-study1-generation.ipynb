{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f89b6b69-90a1-4e9b-951e-df1dd6f4959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import regex\n",
    "import os\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fe69f3-689e-47dd-a358-f7eef30ff653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM server details\n",
    "server1 = \"http://150.140.142.84:1234/v1\" #3070\n",
    "server2 = \"http://150.140.142.83:1234/v1\"\n",
    "\n",
    "llama_client = OpenAI(base_url=server1, api_key=\"lm-studio\")\n",
    "mistral_client = OpenAI(base_url=server2, api_key=\"lm-studio\")\n",
    "\n",
    "llama_model = \"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\"\n",
    "mistral_model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039c4660-a184-4792-a193-56e574338581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prompts used in Studies 1-2\n",
    "prompt1 = '''Write an mobile messaging conversation between two persons. The first person\n",
    "                opens the conversation with either a question or a statement.\n",
    "                The second person responds with the phrase \"{phrase}\".\n",
    "                The first person's phrase must have high contextual relevance to the answer.\n",
    "                The dialogue must consist only of one opening phrase and one response.\n",
    "                Your response must be a single JSON object with two fields: 'opening', 'response'.\n",
    "                You must not include any commentary in your response, ONLY than the JSON object.\n",
    "                    \n",
    "                EXAMPLE PHRASE: \"this is a classroom test\"\n",
    "                EXAMPLE OUTPUT:\n",
    "                {{\n",
    "                    \"opening\":\"what is this document?\",\n",
    "                    \"response\":\"this is a classroom test\",\n",
    "                }}'''\n",
    "\n",
    "prompt2 =  '''Provide a categorisation of \"high\", \"medium\" or \"low\" to assess the logical coherence of the following \n",
    "                two conversations.\n",
    "                Your response must be a single JSON object with two fields: 'conversation1' and 'conversation2'.\n",
    "                You must not include any commentary in your response, ONLY than the JSON object, as per the example \n",
    "                that follows.\n",
    "                \n",
    "                Conversation 1:\n",
    "                Person 1: {mistral_prompt}\n",
    "                Person 2: {original}\n",
    "                \n",
    "                Conversation 2:\n",
    "                Person 1: {llama_prompt}\n",
    "                Person 2: {original}\n",
    "                \n",
    "                EXAMPLE OUTPUT:\n",
    "                {{\"conversation1\": \"medium\", \"conversation2\":\"high\"}}'''\n",
    "\n",
    "prompt3 = '''Write a short narrative of no more than 4 sentences, about two persons, based on the phrase \"{response_phrase}\"\n",
    "                Then, based on that narrative, write a conversation between the two persons in the narrative.\n",
    "                The first person opens the conversation with either a question or a statement, based on the narrative's details.\n",
    "                The second person must respond with the phrase \"{response_phrase}\".\n",
    "                The opening phrase must be written so the response phrase follows logically from it.\n",
    "                The opening phrase MUST NOT contain any of the words in the response phrase.\n",
    "                The dialogue must consist only of one opening phrase and one response.\n",
    "                Your response must be a single JSON object with three fields: 'narrative', 'opening', 'response'.\n",
    "                You must not include any commentary in your response, provide ONLY the JSON object.\n",
    "                You must follow the JSON structure in the example precisely.\n",
    "    \n",
    "                EXAMPLE RESPONSE PHRASE: \"I am in a meeting just now.\"\n",
    "                EXAMPLE GENERATED NARRATIVE: \"Jane wanted to talk to Mark about their daughter, Mary, \n",
    "                        who was facing some trouble at school. However, Mark was at work and engaged \n",
    "                        in a serious meeting.\"\n",
    "                EXAMPLE OUTPUT JSON:\n",
    "                {{\n",
    "                    \"narrative\": \"Jane wanted to talk to Mark about their daughter, Mary, who was facing some trouble at school. However, Mark was at work and engaged in a serious meeting.\", \n",
    "                    \"opening\":\"Can I call you briefly to talk about Mary?\",\n",
    "                    \"response\":\"I am in a meeting just now.\"\n",
    "                }}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012c6ea4-fb78-4323-962e-5aebece11641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to send query to an LLM server\n",
    "def send_query(client, model, prompt, history, temperature):\n",
    "    messages = []\n",
    "    messages.append(\n",
    "        {\"role\": \"system\", \"content\": \"You are an English Natural Language Processing AI tool. Return your answers only as JSON objects\"})\n",
    "    \n",
    "    for item in history:\n",
    "        messages.append(item)\n",
    "    \n",
    "    messages.append(\n",
    "        {\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "      model= model,\n",
    "      messages= messages,\n",
    "      temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# process an LLM response by extracting the JSON object present in it.\n",
    "def procresult(result):\n",
    "    pattern = regex.compile(r'\\{(?:[^{}]|(?R))*\\}')\n",
    "    try:\n",
    "        data = pattern.findall(result)[0]\n",
    "        return json.loads(data)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# read enron metadata\n",
    "def read_enron_md(path):\n",
    "    return pd.read_csv(path, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36d7aaa-bb73-4a1b-8321-f98cfc70aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Study 1 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d60df8-e567-464b-9b0c-2a4b9ca8defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_study1_phrase_pairs(corpus, generators, generator_models, generator_prompt, n_phrases):\n",
    "    pattern = regex.compile(r'\\{(?:[^{}]|(?R))*\\}')\n",
    "\n",
    "    results=[]\n",
    "    \n",
    "    for i in range(0, n_phrases):\n",
    "        try:\n",
    "\n",
    "            prompt = generator_prompt.format(phrase=corpus[i])\n",
    "\n",
    "            # generate from server 1\n",
    "            result1 = send_query(generators[0], generator_models[0], prompt, history=[], temperature=0.8)\n",
    "            proc_result = procresult(result1)\n",
    "            \n",
    "            while proc_result == False:\n",
    "                print('retrying 1',end=\"\\r\")\n",
    "                result1 = send_query(generators[0], generator_models[0], prompt, history=[], temperature=0.8)\n",
    "                proc_result = procresult(result1)\n",
    "            \n",
    "            result1 = proc_result\n",
    "            \n",
    "            # generate from server 2\n",
    "            result2 = send_query(generators[1], generator_models[1], prompt, history=[], temperature=0.8)\n",
    "            proc_result = procresult(result2)\n",
    "            \n",
    "            while proc_result == False:\n",
    "                print('retrying 2',end=\"\\r\")\n",
    "                print(end=LINE_CLEAR)\n",
    "                result2 = send_query(generators[1], generator_models[1], prompt, history=[], temperature=0.8)\n",
    "                proc_result = procresult(result2)\n",
    "            \n",
    "            result2 = proc_result\n",
    "            \n",
    "            #combine results\n",
    "            result={}\n",
    "            result[generator_models[0]]= result1['opening']\n",
    "            result[generator_models[1]]= result2['opening']\n",
    "            result['original'] = corpus[i]\n",
    "\n",
    "            print(str(i+1)+\"/\"+str(n_phrases)+\":\", result[generator_models[0]], '|', result[generator_models[1]], '|' ,\n",
    "                   result['original'])\n",
    "\n",
    "            results.append(result)\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"problem in \"+corpus[i])\n",
    "            print(e)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f22ae2ad-d1a3-4714-8dd7-8fa1bf6c1a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 128 phrases\n",
      "1/3: it's that time of year again, where we traditionally go out for lunch together | Can I join you for lunch? | Are you going to join us for lunch?\n",
      "2/3: I just got my project results back from you, thanks for the quick turnaround. | Can you please provide some information about the project timeline? | Thanks for the quick turnaround.\n",
      "3/3: can you call me tomorrow if possible? | Can you tell me the weather forecast for tomorrow in London? | Please call tomorrow if possible.\n"
     ]
    }
   ],
   "source": [
    "# read enron phrases\n",
    "\n",
    "md_path = 'metadata.txt'\n",
    "df_md = read_enron_md(md_path)\n",
    "corpus = list(df_md[(df_md['mem_count_cer0']>7) & (df_md['words']>=5) & (df_md['words']<=9)]['text'])\n",
    "print('read', len(corpus), 'phrases')\n",
    "\n",
    "# generate phrase pairs (note parameter at end -> just do 3 for test purposes)\n",
    "generated = generate_study1_phrase_pairs(corpus, [llama_client, mistral_client], [llama_model, mistral_model], prompt1, 3)\n",
    "\n",
    "#save as JSON\n",
    "gendict = {}\n",
    "gendict['phrases'] = generated\n",
    "with open('study1-phrasepairs.json', 'w') as f:\n",
    "    f.write(json.dumps(gendict))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29fa5c-8191-4ffa-9c95-36f37ae494de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
