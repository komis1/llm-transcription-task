{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bcc9079-c0e4-49d1-880a-2d853af30fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f4ed96-33d4-408b-845b-79b51fb7afc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732536229.422936 1605594 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6216 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Load universal sentence encoder model\n",
    "# model details: https://www.kaggle.com/models/google/universal-sentence-encoder/tensorFlow2/qa\n",
    "\n",
    "module = hub.load('https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/qa/2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39bbce53-9e00-408e-aff6-e79e8be11b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a USE score for each phrase pair in a df\n",
    "# promptcol: the prompt phrase\n",
    "# origcol: the response phrase (in our case, from the Enron set)\n",
    "# score_col: name of column in the df to save the score\n",
    "\n",
    "def use_rate(df, promptcol, origcol, score_col, use_contexts=False):\n",
    "    use_scores=[]\n",
    "    for idx, row in df.iterrows():\n",
    "        questions=[]\n",
    "        questions.append(row[promptcol])\n",
    "        responses=[row[origcol]]\n",
    "        if not use_contexts:\n",
    "            response_contexts=responses\n",
    "        else:\n",
    "            response_contexts=[]\n",
    "            response_contexts.append(row[promptcol.split(\"_\")[0]+\"_story\"])\n",
    "        try:\n",
    "            question_embeddings = module.signatures['question_encoder'](\n",
    "                    tf.constant(questions))\n",
    "            response_embeddings = module.signatures['response_encoder'](\n",
    "                    input=tf.constant(responses),\n",
    "                    context=tf.constant(response_contexts))\n",
    "            result = np.inner(question_embeddings['outputs'], response_embeddings['outputs'])\n",
    "            #print(result[0][0], np.linalg.norm(question_embeddings['outputs'][0]), np.linalg.norm(response_embeddings['outputs'][0]))\n",
    "            #print(result[0][0])\n",
    "            use_scores.append(result[0][0])\n",
    "        except:\n",
    "            print(row[promptcol], row[origcol])\n",
    "            use_scores.append(-1)\n",
    "    \n",
    "    df[score_col]=use_scores\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b77920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Prompt A</th>\n",
       "      <th>Prompt B</th>\n",
       "      <th>original enron response</th>\n",
       "      <th>Florian</th>\n",
       "      <th>Ioulia</th>\n",
       "      <th>Anna</th>\n",
       "      <th>Annas preference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>315</td>\n",
       "      <td>When can you give me some feedback on my article?</td>\n",
       "      <td>I hope you had time to read my report.</td>\n",
       "      <td>I will take a look at this today.</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192</td>\n",
       "      <td>Sorry I missed your call, how can I help?</td>\n",
       "      <td>Hey, just to let you know, my team made good p...</td>\n",
       "      <td>We need to talk about this month.</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                           Prompt A  \\\n",
       "0  315  When can you give me some feedback on my article?   \n",
       "1  192          Sorry I missed your call, how can I help?   \n",
       "\n",
       "                                            Prompt B  \\\n",
       "0             I hope you had time to read my report.   \n",
       "1  Hey, just to let you know, my team made good p...   \n",
       "\n",
       "             original enron response Florian Ioulia Anna Annas preference  \n",
       "0  I will take a look at this today.       A      A  NaN              NaN  \n",
       "1  We need to talk about this month.       B      A    A           Strong  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read human scoring data\n",
    "# prompts A & B generated by Luis and Andreas\n",
    "# prompt preferences provided by Ioulia and Florian\n",
    "# preference tiebreakers provided by Anna\n",
    "\n",
    "humandf = pd.read_csv('Human Context.csv')\n",
    "humandf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da197a86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan Ava, LA was Socal meeting.\n",
      "nan Or if he comes down I-10N then?\n",
      "nan Joel Ephross on issues reacquiring economic interests/swaps from Jedi2/Whitewing.\n",
      "nan Coal switching to NG and power guys stimulating demand.\n"
     ]
    }
   ],
   "source": [
    "# generate USE scores for generating persons A and B\n",
    "humandf = use_rate(humandf, 'Prompt A', 'original enron response', 'Prompt_A_use_score')\n",
    "humandf = use_rate(humandf, 'Prompt B', 'original enron response', 'Prompt_B_use_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c639eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save USE scores for generating persons A and B\n",
    "humandf.to_csv('Human Context - rated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca7ca88-27de-455d-a2b9-9d63aae801ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse results\n",
    "\n",
    "def select_human_prompt(row):\n",
    "    try:\n",
    "        if pd.isnull(row['Anna']):\n",
    "            if row['Ioulia'] == 'A':\n",
    "                return row['Prompt A']\n",
    "            else:\n",
    "                return row['Prompt B']\n",
    "        else:\n",
    "            return row['Prompt '+row['Anna']]\n",
    "    except:\n",
    "        print(row)\n",
    "\n",
    "def select_human_prompt_use(row):\n",
    "    try:\n",
    "        if row['Prompt_A_use_score']>=row['Prompt_B_use_score']:\n",
    "            return row['Prompt A']\n",
    "        else:\n",
    "            return row['Prompt B']\n",
    "    except:\n",
    "        print(row)\n",
    "\n",
    "def select_human_prompt_use_score(row):\n",
    "    try:\n",
    "        if row['Prompt_A_use_score']>=row['Prompt_B_use_score']:\n",
    "            return row['Prompt_A_use_score']\n",
    "        else:\n",
    "            return row['Prompt_B_use_score']\n",
    "    except:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a858d870-e64c-4dc3-b9f3-a168a68f21ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n"
     ]
    }
   ],
   "source": [
    "#select the \"winning\" prompts based on tie breaker, and then select \"winning\" prompts based on USE score\n",
    "humandf['final_prompt'] = humandf.apply(lambda x: select_human_prompt(x), axis=1)\n",
    "humandf['final_prompt_use'] = humandf.apply(lambda x: select_human_prompt_use(x), axis=1)\n",
    "humandf['final_prompt_use_score'] = humandf.apply(lambda x: select_human_prompt_use_score(x), axis=1)\n",
    "humandf = humandf[humandf['Prompt_B_use_score']!=-1]\n",
    "print(len(humandf))\n",
    "\n",
    "#calculate USE score differences\n",
    "humandf['use_diff'] = humandf.apply(lambda x: abs(x['Prompt_A_use_score'] - x['Prompt_B_use_score']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69579cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### measure alignment of human / USE selected prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1912e26-e7b6-459f-8dfa-8551c9fbce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cases where human  / use selected promt is the same, or where anna's preference is slight or no preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff42222c-8513-4505-865a-e8bbb4e420f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_anna(row):\n",
    "    if not pd.isnull(row['Annas preference']):\n",
    "        if 'no preference' in row['Annas preference']:\n",
    "            return 0\n",
    "        if 'slight' in row['Annas preference']:\n",
    "            return 1\n",
    "        if 'strong' in row['Annas preference']:\n",
    "            return 2 \n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b99db926-a7b9-4d6c-b571-8eda475dfefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Prompt A</th>\n",
       "      <th>Prompt B</th>\n",
       "      <th>original enron response</th>\n",
       "      <th>Florian</th>\n",
       "      <th>Ioulia</th>\n",
       "      <th>Anna</th>\n",
       "      <th>Annas preference</th>\n",
       "      <th>Prompt_A_use_score</th>\n",
       "      <th>Prompt_B_use_score</th>\n",
       "      <th>final_prompt</th>\n",
       "      <th>final_prompt_use</th>\n",
       "      <th>final_prompt_use_score</th>\n",
       "      <th>use_diff</th>\n",
       "      <th>Anna_pref_coded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>315</td>\n",
       "      <td>When can you give me some feedback on my article?</td>\n",
       "      <td>I hope you had time to read my report.</td>\n",
       "      <td>I will take a look at this today.</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.160827</td>\n",
       "      <td>0.064508</td>\n",
       "      <td>When can you give me some feedback on my article?</td>\n",
       "      <td>When can you give me some feedback on my article?</td>\n",
       "      <td>0.160827</td>\n",
       "      <td>0.096319</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192</td>\n",
       "      <td>Sorry I missed your call, how can I help?</td>\n",
       "      <td>Hey, just to let you know, my team made good p...</td>\n",
       "      <td>We need to talk about this month.</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Strong</td>\n",
       "      <td>0.117832</td>\n",
       "      <td>0.056796</td>\n",
       "      <td>Sorry I missed your call, how can I help?</td>\n",
       "      <td>Sorry I missed your call, how can I help?</td>\n",
       "      <td>0.117832</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                           Prompt A  \\\n",
       "0  315  When can you give me some feedback on my article?   \n",
       "1  192          Sorry I missed your call, how can I help?   \n",
       "\n",
       "                                            Prompt B  \\\n",
       "0             I hope you had time to read my report.   \n",
       "1  Hey, just to let you know, my team made good p...   \n",
       "\n",
       "             original enron response Florian Ioulia Anna Annas preference  \\\n",
       "0  I will take a look at this today.       A      A  NaN              NaN   \n",
       "1  We need to talk about this month.       B      A    A           Strong   \n",
       "\n",
       "   Prompt_A_use_score  Prompt_B_use_score  \\\n",
       "0            0.160827            0.064508   \n",
       "1            0.117832            0.056796   \n",
       "\n",
       "                                        final_prompt  \\\n",
       "0  When can you give me some feedback on my article?   \n",
       "1          Sorry I missed your call, how can I help?   \n",
       "\n",
       "                                    final_prompt_use  final_prompt_use_score  \\\n",
       "0  When can you give me some feedback on my article?                0.160827   \n",
       "1          Sorry I missed your call, how can I help?                0.117832   \n",
       "\n",
       "   use_diff  Anna_pref_coded  \n",
       "0  0.096319              4.0  \n",
       "1  0.061035              NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humandf['Anna_pref_coded']= humandf.apply(lambda x: code_anna(x), axis=1)\n",
    "humandf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23aebe9b-0cd0-42ae-9f45-ea7451c9df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cases where human  / use selected prompt is the same, or where\n",
    "# human  / use selected promt is NOT the same, anna's preference exists, \n",
    "# and anna's preference is slight or no preference\n",
    "aligndf = humandf[(humandf['final_prompt']==humandf['final_prompt_use']) | \n",
    "            ((humandf['final_prompt']!=humandf['final_prompt_use']) & (humandf['Anna_pref_coded']<=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d53ef507-b16b-4b97-b931-6977b8256de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment: 0.6686390532544378\n"
     ]
    }
   ],
   "source": [
    "# how \"aligned\" are we?\n",
    "print(\"Alignment:\", len(aligndf)/len(humandf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc1ad98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow some leeway to the use scoring just like Anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d7cdddd-15c5-4d66-96b9-5c70745e5e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0038896562088318373 16.430786675587918\n",
      "1.9973651878070782\n",
      "4.370202769986573\n",
      "Relaxed alignment: 0.8994082840236687\n"
     ]
    }
   ],
   "source": [
    "#calculate prompt diff angle in degrees (from rads)\n",
    "humandf['use_diff_rad'] = humandf.apply(lambda x: np.degrees(abs(np.arccos(x['Prompt_A_use_score'])-np.arccos(x['Prompt_B_use_score']))), axis=1)\n",
    "\n",
    "# check min-max of angle (difference)\n",
    "print(humandf['use_diff_rad'].min(), humandf['use_diff_rad'].max())\n",
    "\n",
    "# find percentile boundaries for \"no pref\", \"slight pref.\" and \"strong pref.\"\n",
    "print(np.percentile(humandf['use_diff_rad'], 33))\n",
    "print(np.percentile(humandf['use_diff_rad'], 66))\n",
    "\n",
    "# relaxed alignment measure\n",
    "# human and use choice is same OR \n",
    "# human and use choice is not same, and anna pref <=1 OR\n",
    "# human and use choice is not same, and use pref difference degrees <= 66th percentile (4.37) \n",
    "aligndf = humandf[\n",
    "            (humandf['final_prompt']==humandf['final_prompt_use']) | \n",
    "            ((humandf['final_prompt']!=humandf['final_prompt_use']) & (humandf['Anna_pref_coded']<=1)) |\n",
    "            ((humandf['final_prompt']!=humandf['final_prompt_use']) & (humandf['use_diff_rad']<=4.37))\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Relaxed alignment:\", len(aligndf)/len(humandf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
